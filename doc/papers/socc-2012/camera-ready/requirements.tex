\section{Requirements}
We had the following requirements while designing Databus.
\begin{enumerate}[I]
\item \emph{No additional point of failure}: 
Since the data source is the source of truth, we want to ensure that the pipeline does not introduce a new point of failure in the architecture. 
\item \emph{Source consistency preservation}: 
We want to preserve the consistency semantics that the source provides. 
This implies needing to support the strongest consistency semantics possible. 
We do not support cross database transactions in the source. 
For transactional sources, to avoid having the subscribers see partial and/or inconsistent data we need to capture:
\begin{itemize}
\item \emph{Transaction boundaries}: a single user's action can trigger atomic updates to multiple rows across tables. 
\item \emph{Commit order}: the exact order in which operations happened on the primary database.
\item \emph{Consistent state}: we can miss changes but cannot provide a change-set that is not consistent at a point in the commit order. e.g. if a row got updated multiple times in quick succession, it is okay to miss an intermediate update, but not okay to miss the last update. 
\end{itemize}
\item \emph{User-space processing}: 
By ``user-space processing'', we refer to the ability to perform the computation triggered by the data change outside the database server. This is in contrast to traditional database triggers that are run in the database server.
Moving the computation to user space has the following benefits:
\begin{itemize}
\item Reduces the load on the database server
\item Avoids affecting the stability of the primary data store
\item Decouples the subscriber implementation from the specifics of the database server implementation
\item Enables independent scaling of the subscribers
\end{itemize}
\item \emph{No assumptions about consumer uptime}: 
Most of the time, consumers are caught up and processing at full speed. However, consumers can have hiccups due to variance in processing time or dependency on external systems, and downtime due to planned maintenance or failures. Sometimes, new consumers get added to increase capacity in the consumer cluster and need to get a recent snapshot of the database. In other cases, consumers might need to re-initialize their entire state by reprocessing the whole data set, e.g. if a key piece of the processing algorithm changes. Therefore, we need to support the capability to go back to an arbitrary point in time.
\item \emph{Isolation between Data-source and consumers}: 
Consumers often perform complex computations that may not allow a single instance to keep up with the data change rate. In those cases, a standard solution is to distribute the computation among multiple instances along some partitioning axis.
Therefore, the pipeline should
\begin{itemize}
\item Allow multiple subscribers to process the changes as a group; i.e. support partitioning;
\item Support different types of partitioning for computation tasks with different scalability requirements;
\item Isolate the source database from the number of subscribers so that increasing the number of the latter should not impact the performance of the former;
\item Isolate the source database from slow or failing subscribers that should not negatively impact the database performance;
\item Isolate subscribers from the operational aspects of the source database: database system choice, partitioning, schema evolution, etc.  
\end{itemize}
\item \emph{Low latency of the pipeline}: Any overhead introduced by the pipeline may introduce risk of inconsistencies, negatively affect performance, or decrease the available time for the asynchronous computations. For example, any latency in updating a secondary index structures (like the previously mentioned LinkedIn social graph index) increases the risk of serving stale or inconsistent data. In the case of replication for read scaling, pipeline latency can lead to higher front-end latencies since more traffic will go to the master for the freshest results. 
\item \emph{Scalable and Highly available}: We need to scale up to thousands of consumers and support thousands of transaction logs while being highly available. 
\end{enumerate}
