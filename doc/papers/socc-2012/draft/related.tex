\section{Related Work}

Before we look at related work in this area, it's important to understand the context in which change data capture is implemented in internet stacks. The common problem in this space is to make the changes to data in online databases available to various specialized systems. There are many different ways this problem gets solved, each with a different set of tradeoffs.

Using a single shared data layer e.g. sharded MySQL usually works for online processing but data must still be made available to Data Warehouses and other offline processing systems such as Hadoop. A different approach is to build on top of infrastructure such as GFS~\cite{gfs} that can be used for both online and offline usecases.
Another common technique is to have the applications or mid-tier services do dual writes to the primary data layer as well a messaging system or write to a messaging layer first and then to the data layer~\cite{gizzard}. This works in situations where data loss and/or consistency problems are acceptable, or there is a single application that the problem can be solved for. Linkedin has a complex ecosystem of specialized systems that solve very specific problems so it's not possible to do this. Also Linkedin has a large number of paying customers who demand high fidelity of their user data and ensuring a good user experience requires that the data pipelines preserve consistency and avoid data loss. Organic growth in the application space also required solving the change data capture problem in a scalable manner, while not sacrificing data consistency and reliability.

\begin{itemize*}
\item \emph{Full featured CDC systems}: Many CDC systems such as Oracle DataGuard~\cite{dataguard} and MySQL replication~\cite{mysqlrepl} are restricted to replicating changes between specific source and destination systems. Other products such as Oracle Streams~\cite{streams} also make the change stream available through user APIs. Systems such as Golden Gate~\cite{goldengate} and Tungsten Replicator~\cite{tungsten} are capable of replicating from different sources to different destinations. But these are designed for usecases where the number of destinations is reasonably low and where consumers have a high uptime. In cases where consumer uptime cannot be guaranteed, seamless snapshotting and arbitrarily long catchup queries must be supported. Most CDC systems are not designed for these scenarios.
 
CDC systems are also designed to be either push based or pull based. In push based systems, the source pushes changes to configured destinations. These are better suited for cases where low latency is desired but generally assume that destinations are reasonably low in number and largely available. Pull based systems on the other hand are better suited for consumers who may not be available at all times but have better support for batched consumption e.g. ETL into data warehouses.
Our usecases require us to support both these usecases efficiently.

\item \emph{Generic messaging systems}
Messaging systems are sometimes used as transport to carry CDC data. There are some key tradeoffs here. Messaging systems such as Kafka~\cite{kafka} and ActiveMQ~\cite{activemq} typically provide publish-subscribe API where publishers are responsible for pushing changes to the messaging system, which is then responsible for guaranteeing the fate of the messages. This typically results in the messaging system acting as a \emph{source-of-truth} system and it's much harder for the system to go back to the publisher and ask for data if there is loss or corruption in the messaging system. Being a \emph{source-of-truth} also leads to the messaging systems to add their own overhead via persistence, internal replication etc. Since CDC systems have an external \emph{source-of-truth}, this overhead is unnecessary.
\end{itemize*}

