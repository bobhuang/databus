\subsection{The Bootstrap Service}

As we have described previously, consumers typically subscribe to changes from the relay, which maintains an in-memory log store. Occasionally, there are situations in which the consumers might fall significantly behind in their processing. This usually happens because of consumer failures, which cause them to be offline for an extended period of time. In other cases, new consumers are launched and they need to bootstrap their initial state before consuming the change log from the relay. 

Possible approaches for dealing with these situations are going back to the source OLTP database and storing extended log at the relays. The first approach is not acceptable since it leads to greatly increased load on the database that is serving online traffic. Besides, getting a consistent snapshot of all the rows in the OLTP database by running a long running query is very difficult. Storing the log at the relay for extended periods of time is not always viable since if the consumer has fallen behind a lot, consuming every change event is likely to be slow and unnecessary. It is much more efficient to catch up using a snapshot which is a compressed representation of the changes i.e. only the latest state of every affected row needs to be consumed. 

Databus implements this functionality using a Bootstrap Service. The bootstrap service consists of three components:
\begin{itemize*}
\item a \emph{bootstrap database}: This has two parts. One is a persistent log store that maintains the change log for an extended time. The other is a snapshot of the data that represents the view of the database at a given point in time. 
\item a \emph{bootstrap producer}: This acts as a regular Databus client, subscribes to the change log from the relay and writes it to the log store in the bootstrap database. 
\item and a \emph{bootstrap applier}: This periodically merges the changes from the log store into the snapshot.
\end{itemize*}

Splitting the responsibilities between the log store writer and the applier has two advantages. First, it keeps the change log persistent over an extended period of time so that consumers that fall behind and do not find changes on the relay can catch up using the log in the bootstrap database. Second, it is able to handle long transactions on the source OLTP database easily since appending to the log is much cheaper than building the snapshot. This ensures that the bootstrap database has enough write throughput to keep up with the source. 

On the consumer side, when a consumer needs to bootstrap, it needs to obtain the change events from both the snapshot and log store so that the combination yields a consistent change set. This is complicated by the fact that the bootstrap producer is updating the snapshot simultaneously. Getting a consistent read of the snapshot by locking the snapshot is not efficient when it is large. Instead the consumer must constantly be allowed to make progress by pulling rows in manageable batch sizes while applier is merging changes from the log store. Since the snapshot data might change across batches, this results in an inconsistent read of the data during the time rows are being read from the snapshot store.  In order to guarantee consistent read at the end of the bootstrapping phase,  bootstrap service uses the following algorithm to deliver changes to the consumer.

\begin{algorithm}
\label{alg:bootstrap}
\caption{Bootstrap Consumption}{bootstrap}{($sources$)} 
\begin{algorithmic}
\STATE $startScn$ = current scn of the bootstrap db
\FOR{$i=0$ to $sources$.length}
\REQUIRE Source $S_{j}$ ($j < i$) is consistent as of $startScn$ \\
\COMMENT{Begin Snapshot phase}
\STATE Get all rows from $S_{i}$ where $rowScn < startScn$ \\
\COMMENT{Can be done in batches}
\STATE $targetScn$ = max SCN of rows in $S_{i}$
\COMMENT{Begin Catchup phase}
\FORALL{source $S$j such that $j \leq i$}
\STATE Get all rows from $S_{j}$ log store from startScn until targetScn
\ENDFOR
\STATE $startScn = targetScn$
\ENDFOR
\end{algorithmic}
\end{algorithm}
