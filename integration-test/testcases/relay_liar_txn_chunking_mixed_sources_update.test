#!/bin/bash
#******************************************************
# set TEST_NAME before calling setup_env.inc
#******************************************************
# simple bootstrap test, generate random event, put that in bootstrap
export TEST_NAME=relay_liar_txn_chunking_mixed_sources_update.test
#******************************************************
# sets up common environmnet variables and 
source setup_env.inc



################### TestCase Description ######################
# In this test, liar_member_relay is slow
# sources and liar_job_relay,liar_news_relay,liar_creatives_relay are 
# their counterparts. The testcases uses a script to control
# the row and scn generation. Both insert and update workload is used. The relay only reads both liar_job and liar_member sources
# Steps:
#  Insert rows in DB such that member has only 1% of rows
#  Setup SCN such that member source has higher SCN than other sources
#  Start 2 relays with SCN such that they start reading from SCN corresponding to the 1st row created for jobs source. Chunking is enabled only for Relay2
#  Start 2 clients. CLient 1 pulls from relay 1 and vice-versa.
#  Wait for max SCN in clients
#  Pause relays. Generate similar load as step 1 but use update instead of insert
#  Now resume the relay. Wait for maxScn in clients
#  Stop services and do log and events check.
############################################################

#***************************************************************************************************************************************
#all ${ALL_CAPS} type vars come from setup_env.inc(except TEST_NAME)...check that file first before introducing any new variables here
#***************************************************************************************************************************************
relay_port_1=${RELAY_PORT_BASE}
let relay_port_2="${RELAY_PORT_BASE} + 1"
client_port_1=${CLIENT_PORT_BASE}
let client_port_2="${CLIENT_PORT_BASE} + 1"
relay_event_dump_file_1=${WORK_DIR_FROM_ROOT}/liar_relay_event_trace_1
relay_event_dump_file_2=${WORK_DIR_FROM_ROOT}/liar_relay_event_trace_2
consumer_1_log=${WORK_DIR_FROM_ROOT}/liar_consumer_1.events
consumer_1_value_log=${WORK_DIR_FROM_ROOT}/liar_consumer_1.values
consumer_2_log=${WORK_DIR_FROM_ROOT}/liar_consumer_2.events
consumer_2_value_log=${WORK_DIR_FROM_ROOT}/liar_consumer_2.values
db_config_file=config/sources-liar.json
db_config_file_2=config/sources-liar-txn-chunk-2.json
relay1_maxscn_dir=${VIEW_ROOT}/${WORK_DIR_FROM_ROOT}/maxScn1
relay2_maxscn_dir=${VIEW_ROOT}/${WORK_DIR_FROM_ROOT}/maxScn2
relay_1_server_log=${VIEW_ROOT}/${LOG_DIR_FROM_ROOT}/liar_relay_${relay_port_1}_`date +%Y_%m_%d_%H_%M_%S`.log
relay_2_server_log=${VIEW_ROOT}/${LOG_DIR_FROM_ROOT}/liar_relay_${relay_port_2}_`date +%Y_%m_%d_%H_%M_%S`.log

#Cleanup Src DB 
$SCRIPT_DIR/setup_liar_rows_scn.sh liar/liar@DB unload 

#Setup some rows before starting relays
$SCRIPT_DIR/setup_liar_rows_scn.sh liar/liar@DB load 1 5000 job
$SCRIPT_DIR/setup_liar_rows_scn.sh liar/liar@DB load 1 5000 creative
$SCRIPT_DIR/setup_liar_rows_scn.sh liar/liar@DB load 1 5000 news
$SCRIPT_DIR/setup_liar_rows_scn.sh liar/liar@DB load 1 15 member
$SCRIPT_DIR/setup_liar_rows_scn.sh liar/liar@DB updatescn 20000 0

# Get current Max SCN
DB_MAXSCN=`echo 'select max(scn) from sy$txlog;' | sqlplus -S liar/liar@DB  | tail -2 | head -1 | perl -lane '{my $a = $_; $a =~ s/\D*(\d+)\D*/$1/g; print "$a";} '`;

let relay_start_Scn="${DB_MAXSCN} - 20000" # will ensure start_scn is < SCN(first row for news (which is the first source to be inserted)).
echo "DB Max SCN is : ${DB_MAXSCN} , Relay_start_Scn : ${relay_start_Scn}";

# 10M buffer , event dump file relay 1
$SCRIPT_DIR/dbus2_driver.py -c db_relay -o start --db_config_file=${db_config_file} --logfile=${relay_1_server_log} --cmdline_props="databus.relay.eventBuffer.maxSize=10240000;databus.relay.eventBuffer.scnIndexSize=10240;databus.relay.eventBuffer.trace.option=file;databus.relay.eventBuffer.trace.filename=${relay_event_dump_file_1};databus.relay.eventBuffer.trace.appendOnly=false;databus.relay.container.httpPort=${relay_port_1};databus.relay.container.jmx.jmxServicePort=19998;databus.relay.dataSources.sequenceNumbersHandler.file.scnDir=${relay1_maxscn_dir}" --jvm_direct_memory_size=30M  -l config/client-log4j2file.properties.debug

# 10M buffer , event dump file relay 2
$SCRIPT_DIR/dbus2_driver.py -c db_relay -o start --db_config_file=${db_config_file_2} --logfile=${relay_2_server_log} --cmdline_props="databus.relay.eventBuffer.maxSize=10240000;databus.relay.eventBuffer.scnIndexSize=10240;databus.relay.eventBuffer.trace.option=file;databus.relay.eventBuffer.trace.filename=${relay_event_dump_file_2};databus.relay.eventBuffer.trace.appendOnly=false;databus.relay.container.httpPort=${relay_port_2};databus.relay.container.jmx.jmxServicePort=19999;databus.relay.dataSources.sequenceNumbersHandler.file.scnDir=${relay2_maxscn_dir}" --jvm_direct_memory_size=30M  -l config/client-log4j2file.properties.debug

#Set startScn
$SCRIPT_DIR/dbus2_gen_event.py --db_gen -s liar  --db_config_file=${db_config_file} --from_scn=${relay_start_Scn} --server_port=${relay_port_1} --debug
sleep 5
$SCRIPT_DIR/dbus2_gen_event.py --db_gen -s liar  --db_config_file=${db_config_file_2} --from_scn=${relay_start_Scn} --server_port=${relay_port_2} --debug

# start consumer1
$SCRIPT_DIR/dbus2_driver.py -c liar_consumer -o start --dump_file=${consumer_1_log}  --value_file=${consumer_1_value_log} --http_port=${client_port_1} --relay_port=${relay_port_1} --cmdline_props="databus.client.connectionDefaults.eventBuffer.maxSize=10240000;databus.client.connectionDefaults.eventBuffer.scnIndexSize=1024000;databus.client.connectionDefaults.eventBuffer.readBufferSize=1024000;databus.client.checkpointPersistence.fileSystem.rootDirectory=./liarclient-checkpoints1;databus.client.checkpointPersistence.clearBeforeUse=true;databus.client.connectionDefaults.enablePullerMessageQueueLogging=true;"

# start consumer2
$SCRIPT_DIR/dbus2_driver.py -c liar_consumer -o start --dump_file=${consumer_2_log}  --value_file=${consumer_2_value_log} --http_port=${client_port_2} --relay_port=${relay_port_2} --cmdline_props="databus.client.connectionDefaults.eventBuffer.maxSize=10240000;databus.client.connectionDefaults.eventBuffer.scnIndexSize=1024000;databus.client.connectionDefaults.eventBuffer.readBufferSize=1024000;databus.client.checkpointPersistence.fileSystem.rootDirectory=./liarclient-checkpoints2;databus.client.checkpointPersistence.clearBeforeUse=true;databus.client.connectionDefaults.enablePullerMessageQueueLogging=true;"

echo "Sleep for 60 sec"
sleep 60;

#wait for both consumer 1 and consumer 2 to catchup
$SCRIPT_DIR/dbus2_driver.py -c liar_consumer -o wait_event --timeout=60 --http_port=${client_port_1} --relay_port=${relay_port_1}
$SCRIPT_DIR/dbus2_driver.py -c liar_consumer -o wait_event --timeout=60 --http_port=${client_port_2} --relay_port=${relay_port_2}

echo "Verifying if TXN Chunking has been enabled"
stat_txt="Verifying if TXN Chunking has been enabled"
grep "Enabling chunking for source" ${relay_2_server_log} | wc -l | perl -lane '{ my $a = $_; chomp($a); if ($a < 1 ) { print $a; exit 1; } else { exit 0;} }'
source report_pass_fail.inc

echo "Verifying if TXN Chunking has been disabled"
stat_txt="Verifying if TXN Chunking has been disabled"
grep "Disabling chunking for source" ${relay_2_server_log} | wc -l | perl -lane '{ my $a = $_; chomp($a); if ($a < 1 ) { print $a; exit 1; } else { exit 0;} }'
source report_pass_fail.inc

echo "Pausing Chunked Relay"
$SCRIPT_DIR/dbus2_gen_event.py --db_gen --suspend_gen --src_ids=20,21 --server_port=${relay_port_2} --debug

echo "Sleep 5"
sleep 5

echo "Generating more events"
$SCRIPT_DIR/setup_liar_rows_scn.sh liar/liar@DB update 1 5000 job
$SCRIPT_DIR/setup_liar_rows_scn.sh liar/liar@DB update 1 5000 creative
$SCRIPT_DIR/setup_liar_rows_scn.sh liar/liar@DB update 1 5000 news
$SCRIPT_DIR/setup_liar_rows_scn.sh liar/liar@DB update 1 15 member
$SCRIPT_DIR/setup_liar_rows_scn.sh liar/liar@DB updatescn 40000 15016

echo "Sleep for 10 sec"
sleep 10;

echo "Resetting Catchup SCN" 
resetOutput=`curl "http://localhost:9001/testOracleProducer/resetCatchupScn"`
echo "Resetting Catchup SCN output : ${resetOutput}";

echo "Sleep for 10 sec"
sleep 10;

echo "Resuming Chunked Relay"
$SCRIPT_DIR/dbus2_gen_event.py --db_gen --src_ids=20,21 --resume_gen --server_port=${relay_port_2} --debug

echo "Sleep for 60 sec"
sleep 60;

#wait for both consumer 1 and consumer 2 to catchup
$SCRIPT_DIR/dbus2_driver.py -c liar_consumer -o wait_event --timeout=60 --http_port=${client_port_1} --relay_port=${relay_port_1}
$SCRIPT_DIR/dbus2_driver.py -c liar_consumer -o wait_event --timeout=60 --http_port=${client_port_2} --relay_port=${relay_port_2}

echo "Verifying if TXN Chunking has been enabled again"
stat_txt="Test $0 Verifying if TXN Chunking has been enabled"
grep "Enabling chunking for source" ${relay_2_server_log} | wc -l | perl -lane '{ my $a = $_; chomp($a); if ($a < 2 ) { print $a; exit 1; } else { exit 0;} }'
source report_pass_fail.inc

echo "Verifying if TXN Chunking has been disabled again"
stat_txt="Verifying if TXN Chunking has been disabled again"
grep "Disabling chunking for source" ${relay_2_server_log} | wc -l | perl -lane '{ my $a = $_; chomp($a); if ($a < 2 ) { print $a; exit 1; } else { exit 0;} }'
source report_pass_fail.inc

echo "Verifying if Regular Query was used for fetching liar_job_relay"
stat_txt="Verifying if Regular Query was used for fetching liar_job_relay"
grep  "select /\*+ first_rows LEADING[(]tx[)] \*/ liar.sync_core.getScn[(]tx.scn, tx.ora_rowscn[)] scn, tx.ts event_timestamp, src.\* from liar.sy\$liar_job_relay_2 src, liar.sy\$txlog tx where src.txn=tx.txn and tx.scn > [?] and tx.ora_rowscn > [?]; skipInfinityScn=false" ${relay_2_server_log} | wc -l | perl -lane '{ my $a = $_; chomp($a); if ($a <= 0 ) { print $a; exit 1; } else { exit 0;} }'
source report_pass_fail.inc

echo "Verifying if Regular Query was used for fetching liar_member_relay"
stat_txt="Verifying if Regular Query was used for fetching liar_member_relay"
grep  "select /\*+ first_rows LEADING[(]tx[)] \*/ liar.sync_core.getScn[(]tx.scn, tx.ora_rowscn[)] scn, tx.ts event_timestamp, src.\* from liar.sy\$liar_member_relay src, liar.sy\$txlog tx where src.txn=tx.txn and tx.scn > [?] and tx.ora_rowscn > [?]; skipInfinityScn=false" ${relay_2_server_log} | wc -l | perl -lane '{ my $a = $_; chomp($a); if ($a <= 0 ) { print $a; exit 1; } else { exit 0;} }'
source report_pass_fail.inc

echo "Verifying if TXN Chunked Query was used for fetching liar_job_relay"
stat_txt="Verifying if TXN Chunked Query was used for fetching liar_job_relay"
grep 'SELECT scn, event_timestamp, src.\* FROM liar.sy\$liar_job_relay_2 src, [(] SELECT /\*+ first_rows LEADING[(]tx[)] cardinality[(]tx,1[)] \*/ liar.sync_core.getScn[(]tx.scn, tx.ora_rowscn[)] scn, tx.ts event_timestamp, tx.txn, row_number[(][)] OVER [(]ORDER BY TX.SCN[)] r FROM liar.sy\$txlog tx WHERE tx.scn > [?] AND tx.ora_rowscn > [?] AND tx.scn < 9999999999999999999999999999[)] t WHERE src.txn = t.txn AND r<= [?] ORDER BY r ; skipInfinityScn=false' ${relay_2_server_log} | wc -l | perl -lane '{ my $a = $_; chomp($a); if ($a <= 0 ) { print $a; exit 1; } else { exit 0;} }'
source report_pass_fail.inc

echo "Verifying if TXN Chunked Query was used for fetching liar_member_relay"
stat_txt="Verifying if TXN Chunked Query was used for fetching liar_member_relay"
grep 'SELECT scn, event_timestamp, src.\* FROM liar.sy\$liar_member_relay src, [(] SELECT /\*+ first_rows LEADING[(]tx[)] cardinality[(]tx,1[)] \*/ liar.sync_core.getScn[(]tx.scn, tx.ora_rowscn[)] scn, tx.ts event_timestamp, tx.txn, row_number[(][)] OVER [(]ORDER BY TX.SCN[)] r FROM liar.sy\$txlog tx WHERE tx.scn > [?] AND tx.ora_rowscn > [?] AND tx.scn < 9999999999999999999999999999[)] t WHERE src.txn = t.txn AND r<= [?] ORDER BY r ; skipInfinityScn=false' ${relay_2_server_log} | wc -l | perl -lane '{ my $a = $_; chomp($a); if ($a <= 0 ) { print $a; exit 1; } else { exit 0;} }'
source report_pass_fail.inc

echo "Verifying if SCN Chunked Query was NOT used for fetching liar_job_relay"
stat_txt="Verifying if SCN Chunked Query was NOT used for fetching liar_job_relay"
grep  'SELECT /\*+ first_rows LEADING[(]tx[)] \*/ sync_core.getScn[(]tx.scn, tx.ora_rowscn[)] scn, tx.ts event_timestamp, src.* FROM sy\$liar_job_relay_2 src, sy\$txlog tx WHERE src.txn=tx.txn AND tx.scn > ? AND tx.ora_rowscn > [?] AND  tx.ora_rowscn <= [?]; skipInfinityScn=false' ${relay_2_server_log} | wc -l | perl -lane '{ my $a = $_; chomp($a); if ($a > 0 ) { print $a; exit 1; } else { exit 0;} }'
source report_pass_fail.inc

echo "Verifying if SCN Chunked Query was NOT used for fetching liar_member_relay"
stat_txt="Verifying if SCN Chunked Query was NOT used for fetching liar_member_relay"
grep  'SELECT /\*+ first_rows LEADING[(]tx[)] \*/ sync_core.getScn[(]tx.scn, tx.ora_rowscn[)] scn, tx.ts event_timestamp, src.* FROM sy\$liar_member_relay src, sy\$txlog tx WHERE src.txn=tx.txn AND tx.scn > ? AND tx.ora_rowscn > [?] AND  tx.ora_rowscn <= [?]; skipInfinityScn=false' ${relay_2_server_log} | wc -l | perl -lane '{ my $a = $_; chomp($a); if ($a > 0 ) { print $a; exit 1; } else { exit 0;} }'
source report_pass_fail.inc


stat_txt="Compare Relay1 and Consumer1 log"
echo ==$stat_txt :Compare JSON
$SCRIPT_DIR/dbus2_json_compare.py -s -c ${relay_event_dump_file_1} ${consumer_1_log} --fk_src_order=20,21
source report_pass_fail.inc

stat_txt="Compare Relay2 and Consumer2 log"
echo ==$stat_txt :Compare JSON
$SCRIPT_DIR/dbus2_json_compare.py -s -c ${relay_event_dump_file_2} ${consumer_2_log} --fk_src_order=20,21
source report_pass_fail.inc

#compare result
stat_txt="Compare DB and consumer1 log"
echo ==$stat_txt 
$SCRIPT_DIR/dbus2_json_compare.py -s -c --sort_key --db_src_ids=20,21 --db_config_file=${db_config_file} ${consumer_1_value_log}
source report_pass_fail.inc

#compare result
stat_txt="Compare DB and consumer2 log"
echo ==$stat_txt 
$SCRIPT_DIR/dbus2_json_compare.py -s -c --sort_key --db_src_ids=20,21 --db_config_file=${db_config_file} ${consumer_2_value_log}
source report_pass_fail.inc

# stop
stat_txt="Stop Consumer"
$SCRIPT_DIR/dbus2_driver.py -c liar_consumer -o stop
source report_pass_fail.inc

$SCRIPT_DIR/dbus2_driver.py -c db_relay -o stop

final_report=1
stat_txt="Relay Pull Thread Validation"
cat ${VIEW_ROOT}/${WORK_DIR_FROM_ROOT}/*log* | perl $SCRIPT_DIR/validateRelayPullerMessageQueue.pl
source report_pass_fail.inc
exit $all_stat
