#!/bin/bash
#******************************************************
# Test Case to test the checkpointing progress of clients when clients get restarted
# In this case, the storage nodes maintain the same role (Master/slave) and no rebalancing occurs
# StorageNode Setup with 1 Partition EspressoDB with 1 Master and 1 Slave
# Master - Node 1
# Slave - Node 2
#  1. Setup Storage Nodes and cluster
#  2. Start Relay Cluster and Start Relays
#  3. Start 2 clients client1 and client2 with a subscription to EspressoDB partition
#  4. Generate Data to the Storage Node
#  5. Verify Data at the Clients and their states
#  6. Stop Relays
#  7. Verify Client is suspended as there are no more relays ( this ensures dynamic notification happens as without this 
#                                          client will not go to Suspended state (since retry is set to -1 (never stop)))
#  8. Start Relays 
#  9. Verify clients are back to active.
# 10. Generate Data to the Storage Node
# 11. Verify all the data is present at the clients ( compare with both relay and data file )
# 12. Now restart the clients (with checkpoint.clearBrforeUse=false 
# 13. Genereate more data to the storage node and verify all the data is present at the clients
# 
# set TEST_NAME before calling setup_env.inc
#******************************************************
export TEST_NAME=espresso_client_5_3_2_test11

# sets up common environmnet variables and 
source setup_env2.inc
#***************************************************************************************************************************************
#all ${ALL_CAPS} type vars come from setup_env.inc(except TEST_NAME)...check that file first before introducing any new variables here
#***************************************************************************************************************************************
espressoDB_list=EspressoDB  	#Can be a comma-separated list
espressoDB_range=1     	#Can be a comma-separated list
espressoDB_replicas=2 	#Can be a comma-separated list
clusterName=relayIntTemp
jvm_direct_memory="-XX:MaxDirectMemorySize=200m"
jvm_min_heap="-Xms100m"
jvm_max_heap="-Xmx100m"
jvm_gc_args="-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:-CMSParallelRemarkEnabled -XX:MaxTenuringThreshold=1 -XX:SurvivorRatio=3 -XX:CMSInitiatingOccupancyFraction=85 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintTenuringDistribution"
jvm_args="${jvm_direct_memory} ${jvm_min_heap} ${jvm_max_heap} ${jvm_new_size} ${gvm_gc_args}"
let client_port_1="${CLIENT_PORT_BASE}+2"
let client_port_2="${CLIENT_PORT_BASE}+3"
relay_event_trace_1=${WORK_DIR_FROM_ROOT}/espresso_relay_event_trace
relay_event_trace_merged=${WORK_DIR_FROM_ROOT}/espresso_relay_event_trace_merged
relay_event_trace_merged_2=${WORK_DIR_FROM_ROOT}/espresso_relay_event_trace_merged_2
relay_event_trace_merged_3=${WORK_DIR_FROM_ROOT}/espresso_relay_event_trace_merged_3
relay_event_trace_merged_4=${WORK_DIR_FROM_ROOT}/espresso_relay_event_trace_merged_4
relay_event_trace_merged_5=${WORK_DIR_FROM_ROOT}/espresso_relay_event_trace_merged_5
consumer_1_log_1=${WORK_DIR_FROM_ROOT}/espresso_consumer_1.events.1
consumer_1_value_log=${WORK_DIR_FROM_ROOT}/espresso_consumer_1.values
consumer_2_log_1=${WORK_DIR_FROM_ROOT}/espresso_consumer_2.events.1
consumer_2_value_log=${WORK_DIR_FROM_ROOT}/espresso_consumer_2.values
consumer_1_cp_dir=${WORK_DIR_FROM_ROOT}/ckpt/1
consumer_2_cp_dir=${WORK_DIR_FROM_ROOT}/ckpt/2
consumer_1_cp_dir_from_root=${VIEW_ROOT}/${WORK_DIR_FROM_ROOT}/ckpt/1
consumer_2_cp_dir_from_root=${VIEW_ROOT}/${WORK_DIR_FROM_ROOT}/ckpt/2
espresso_conf_dir=${CONFIG_DIR}/espresso
espressoDB_config_file_base=${espresso_conf_dir}/json/ppart__es_EspressoDB
client_subs=espresso://MASTER/EspressoDB/0/*
data_root=${VIEW_ROOT}/integration-test/testcases/espresso/data
data_file=${data_root}/EspressoDB_Random1.dat

relay1_maxscn_dir=${VIEW_ROOT}/${WORK_DIR_FROM_ROOT}/maxScn1
relay2_maxscn_dir=${VIEW_ROOT}/${WORK_DIR_FROM_ROOT}/maxScn2

######### SETUP #########
# Uncomment this if you want the relay cluster and storage node clusters to be hosted on separate zookeeper instances
# export RELAY_ZK_PORT=2183

# Setup espresso components
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o stop
$SCRIPT_DIR/dbus2_driver.py -n ${TEST_NAME} -c cluster_manager -o setup --db_list "${espressoDB_list}" --db_range "${espressoDB_range}" --db_replicas "${espressoDB_replicas}" --zookeeper_server_hosts=${ZK_HOST} --zookeeper_server_ports=${ZK_PORT} --helix_clustername=${STORAGE_NODE_CLUSTER} --relay_zookeeper_server_hosts=${RELAY_ZK_HOST} --relay_zookeeper_server_ports=${RELAY_ZK_PORT}
echo '***** done setting up espresso components *****'


# Create the checkpoint directory
if [ ! -d ${consumer_1_cp_dir_from_root} ]; then
    mkdir -p ${consumer_1_cp_dir_from_root}
fi 

#reset the relay maxscn
if [ ! -d ${relay1_maxscn_dir} ]; then
    mkdir -p ${relay1_maxscn_dir}
else
   rm -rf ${relay1_maxscn_dir}
   mkdir -p ${relay1_maxscn_dir}
fi 

if [ ! -d ${relay2_maxscn_dir} ]; then
    mkdir -p ${relay2_maxscn_dir}
else
   rm -rf ${relay2_maxscn_dir}
   mkdir -p ${relay2_maxscn_dir}
fi 

# Setup Relay Cluster
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o invoke --cluster_manager_props="--addCluster ${clusterName} --zkSvr ${RELAY_ZK_HOST}:${RELAY_ZK_PORT}"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o invoke --cluster_manager_props="--listClusters --zkSvr ${RELAY_ZK_HOST}:${RELAY_ZK_PORT}" 
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o invoke --cluster_manager_props="--addNode ${clusterName} localhost:${relay1Port} --zkSvr ${RELAY_ZK_HOST}:${RELAY_ZK_PORT}"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o invoke --cluster_manager_props="--addNode ${clusterName} localhost:${relay2Port} --zkSvr ${RELAY_ZK_HOST}:${RELAY_ZK_PORT}"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o invoke --cluster_manager_props="--addResource ${clusterName} relayLeaderStandby 1 LeaderStandby --zkSvr ${RELAY_ZK_HOST}:${RELAY_ZK_PORT}"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o invoke --cluster_manager_props="--rebalance ${clusterName} relayLeaderStandby 2 --zkSvr ${RELAY_ZK_HOST}:${RELAY_ZK_PORT}" 
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o invoke --cluster_manager_props="--addResource ${clusterName} ${espressoDB_list} ${espressoDB_range} OnlineOffline --zkSvr ${RELAY_ZK_HOST}:${RELAY_ZK_PORT}" 
#$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_admin_client -o start --zookeeper_server_hosts=${RELAY_ZK_HOST} --zookeeper_server_ports=${RELAY_ZK_PORT}
#$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_admin_client -o checkLeader --cluster_name ${clusterName} --node_name ${relay1Name}
source report_pass_fail.inc
echo '***** done creating relay cluster ***** '

#Start helix controller for relay cluster
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o start --helix_clustername=${clusterName} --zookeeper_server_hosts=${RELAY_ZK_HOST} --zookeeper_server_ports=${RELAY_ZK_PORT}
echo '***** done starting helix controller for relay cluster ***** '

#start the 2 relays. Configure them to recieve events from the EspressoDB schema. Register it to listen to the EV of the "DevCluster_Dbus" Espresso Storage Node Cluster 
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_relay -o start --db_config_file=${espressoDB_config_file_base} --db_config_file_range=${espressoDB_range} --cmdline_props="databus.relay.eventBuffer.trace.filename=${relay_event_trace_1}_${relay1Name};databus.relay.eventBuffer.trace.appendOnly=false;databus.relay.container.httpPort=${relay1Port};databus.relay.clusterManager.instanceName=${relay1Name};databus.relay.container.tcp.port=${relay1TCPPort};databus.relay.clusterManager.relayClusterName=${clusterName};databus.relay.clusterManager.relayZkConnectString=${RELAY_ZK_HOST}:${RELAY_ZK_PORT};databus.relay.dataSources.sequenceNumbersHandler.file.scnDir=${relay1_maxscn_dir}" -p ${espresso_conf_dir}/espresso_relay_cluster_manager_local.properties -l ${espresso_conf_dir}/espresso_relay_log4j.properties --jvm_args="${jvm_args}" 
echo '***** done creating first relay ***** '

$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_relay -o start --db_config_file=${espressoDB_config_file_base} --db_config_file_range=${espressoDB_range} --cmdline_props="databus.relay.eventBuffer.trace.filename=${relay_event_trace_1}_${relay2Name};databus.relay.eventBuffer.trace.appendOnly=false;databus.relay.container.httpPort=${relay2Port};databus.relay.clusterManager.instanceName=${relay2Name};databus.relay.container.tcp.port=${relay2TCPPort};databus.relay.clusterManager.relayClusterName=${clusterName};databus.relay.clusterManager.relayZkConnectString=${RELAY_ZK_HOST}:${RELAY_ZK_PORT};databus.relay.dataSources.sequenceNumbersHandler.file.scnDir=${relay2_maxscn_dir}" -p ${espresso_conf_dir}/espresso_relay_cluster_manager_local.properties -l ${espresso_conf_dir}/espresso_relay_log4j2.properties --jvm_args="${jvm_args}"
echo '***** done creating second relay ***** '

# Sleep for the relay to startup
sleep 90

# Write events
$SCRIPT_DIR/dbus2_gen_event.py --espresso_gen --espresso_data_file=${data_file} --espresso_db_name=${espressoDB_list} --espresso_table_name=IdNamePair --num_events=100 --event_per_sec=100 --espresso_host=${EspressoRouter} --espresso_port=${EspressoRouterPort} 

# Sleep for the relay to catchup
sleep 10

#start the client and register it to receive events for EspressoDB all partitions
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o start --value_file=${consumer_1_value_log} --cmdline_props="databus.espresso.client.checkpointPersistence.fileSystem.rootDirectory=${consumer_1_cp_dir_from_root};databus.espresso.client.checkpointPersistence.clearBeforeUse=true;databus.espresso.client.connectionDefaults.eventBuffer.trace.filename=${consumer_1_log_1};databus.espresso.client.container.httpPort=${client_port_1};databus.espresso.client.clusterManager.relayClusterName=${clusterName};databus.espresso.client.clusterManager.enableDynamic=true;databus.espresso.client.subscriptions=${client_subs};databus.espresso.client.clusterManager.relayZkConnectString=${RELAY_ZK_HOST}:${RELAY_ZK_PORT};databus.espresso.client.connectionDefaults.dispatcherRetries.maxSleep=6000" -p ${espresso_conf_dir}/espresso_client_53.properties -l ${espresso_conf_dir}/espresso_client_log4j.properties  --jvm_args="${jvm_args}" 
echo '***** done creating first client ***** '
#start the client and register it to receive events for EspressoDB all partitions
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o start --value_file=${consumer_2_value_log} --cmdline_props="databus.espresso.client.checkpointPersistence.fileSystem.rootDirectory=${consumer_2_cp_dir_from_root};databus.espresso.client.checkpointPersistence.clearBeforeUse=true;databus.espresso.client.connectionDefaults.eventBuffer.trace.filename=${consumer_2_log_1};databus.espresso.client.container.httpPort=${client_port_2};databus.espresso.client.clusterManager.relayClusterName=${clusterName};databus.espresso.client.clusterManager.enableDynamic=true;databus.espresso.client.subscriptions=${client_subs};databus.espresso.client.clusterManager.relayZkConnectString=${RELAY_ZK_HOST}:${RELAY_ZK_PORT};databus.espresso.client.connectionDefaults.dispatcherRetries.maxSleep=6000" -p ${espresso_conf_dir}/espresso_client_53.properties -l ${espresso_conf_dir}/espresso_client_log4j.properties  --jvm_args="${jvm_args}" 
echo '***** done creating second client ***** '

# sleep for the client to start the puller threads
sleep 15

#Verify number of Registrations in client == number of partitions we specified
echo ==Verify Registrations for client 1
stat_txt="Verify Registrations for client 1"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o verifyRegistrations --expected_num_registrations=1 --client_base_port_list=${client_port_1}
source report_pass_fail.inc
echo ==Verify Registrations for client 2
stat_txt="Verify Registrations for client 2"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o verifyRegistrations --expected_num_registrations=1 --client_base_port_list=${client_port_2}
source report_pass_fail.inc

# Get RegId
regId1=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getRegistrationsByPhysicalPartition --expected_num_registrations=1 --client_base_port_list=${client_port_1} --db_name=${espressoDB_list} --partition_num=0`
echo "RegId1 is : ${regId1}"
regId2=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getRegistrationsByPhysicalPartition --expected_num_registrations=1 --client_base_port_list=${client_port_2} --db_name=${espressoDB_list} --partition_num=0`
echo "RegId2 is : ${regId2}"

# check if Relay Puller is active
echo ==Check If Relay Puller is active for client 1
stat_txt="Test if Relay Puller Active for client 1"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o isRelayPullerActive --client_base_port_list=${client_port_1} --reg_id=${regId1} --debug
source report_pass_fail.inc
echo ==Check If Relay Puller is active for client 2
stat_txt="Test if Relay Puller Active for client 2"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o isRelayPullerActive --client_base_port_list=${client_port_2} --reg_id=${regId2} --debug
source report_pass_fail.inc


curr_relay1=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getCurrentRelay  --client_base_port_list=${client_port_1} --reg_id=${regId1}`
echo "Current Relay for client 1 is : ${curr_relay1}"
curr_relay1=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getCurrentRelay  --client_base_port_list=${client_port_2} --reg_id=${regId2}`
echo "Current Relay for client 1 is : ${curr_relay2}"

## Wait for client catch up
echo ==Waiting for client1 to catchup
stat_txt="Waiting for client2 to catchup "
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o wait_event_53  --expected_num_registrations=1 --db_list=${espressoDB_list} --db_range=1 --client_base_port_list=${client_port_1} --reg_id=${regId1} --debug
source report_pass_fail.inc
echo ==Waiting for client2 to catchup
stat_txt="Waiting for client2 to catchup "
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o wait_event_53 --expected_num_registrations=1 --db_list=${espressoDB_list} --db_range=1 --client_base_port_list=${client_port_2} --reg_id=${regId2} --debug
source report_pass_fail.inc

## Wait for 30 sec
sleep 30

#compare relay and client event logs
stat_txt="Test $0: Compare relay log and client1 log (before relay restart)"
echo "Test : Compare relay log and client1 log (before relay restart)"
#$SCRIPT_DIR/dbus2_json_compare.py --espresso_compare --file1=${file1} --file2=${consumer_1_log_1} --db_list=${espressoDB_list} --db_range=1
$SCRIPT_DIR/dbus2_json_compare.py --espresso_cluster_compare --file1=${relay_event_trace_1} --file2=${consumer_1_log_1} --db_list=${espressoDB_list} --db_partitions=0 --client_host=${CLIENT_HOST} --client_port=${client_port_1} --master=True --slave=False --append=False --debug
source report_pass_fail.inc

echo "Test : Check the merged relay log"
stat_txt="Test : Check the merged relay log"
$SCRIPT_DIR/dbus2_json_compare.py --cluster_merge --in=${relay_event_trace_1} --out=${relay_event_trace_merged_2} --db_list=${espressoDB_list} --db_range=1  --client_host=${CLIENT_HOST} --client_port=${client_port_1} --append=False --expected_event_count=100 --debug
source report_pass_fail.inc

#compare relay and client event logs
stat_txt="Test $0: Compare relay log and client2 log (before relay restart)"
echo "Test : Compare relay log and client2 log (before relay restart)"
$SCRIPT_DIR/dbus2_json_compare.py --espresso_cluster_compare --file1=${relay_event_trace_1} --file2=${consumer_2_log_1} --db_list=${espressoDB_list} --db_partitions=0 --client_host=${CLIENT_HOST} --client_port=${client_port_2} --master=True --slave=False --compare_without_merge=True --debug
source report_pass_fail.inc

## Now stop the relay  and wait for 30 secs
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_relay -o stop

## Wait for 60 sec (need more than 30 sec here)
sleep 60

curr_time=`date`
echo "Current Time :${curr_time}"

## Expect the relay to be not active
# check if Relay Puller is active
echo ==Check If Relay Puller is inactive at client 1
stat_txt="Test if Relay Puller is Inactive at client 1"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o isRelayPullerInactive --client_base_port_list=${client_port_1} --reg_id=${regId1} --debug
source report_pass_fail.inc
echo ==Check If Relay Puller is inactive at client 2
stat_txt="Test if Relay Puller is Inactive at client 2"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o isRelayPullerInactive --client_base_port_list=${client_port_2} --reg_id=${regId2} --debug
source report_pass_fail.inc



#start the 2 relays. Configure them to recieve events from the EspressoDB schema. Register it to listen to the EV of the "DevCluster_Dbus" Espresso Storage Node Cluster 
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_relay -o start --db_config_file=${espressoDB_config_file_base} --db_config_file_range=${espressoDB_range} --cmdline_props="databus.relay.eventBuffer.trace.filename=${relay_event_trace_1}_${relay1Name};databus.relay.eventBuffer.trace.appendOnly=false;databus.relay.container.httpPort=${relay1Port};databus.relay.clusterManager.instanceName=${relay1Name};databus.relay.container.tcp.port=${relay1TCPPort};databus.relay.clusterManager.relayClusterName=${clusterName};databus.relay.clusterManager.relayZkConnectString=${RELAY_ZK_HOST}:${RELAY_ZK_PORT};databus.relay.dataSources.sequenceNumbersHandler.file.scnDir=${relay1_maxscn_dir}" -p ${espresso_conf_dir}/espresso_relay_cluster_manager_local.properties -l ${espresso_conf_dir}/espresso_relay_log4j.properties --jvm_args="${jvm_args}" 
echo '***** done creating first relay ***** '

$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_relay -o start --db_config_file=${espressoDB_config_file_base} --db_config_file_range=${espressoDB_range} --cmdline_props="databus.relay.eventBuffer.trace.filename=${relay_event_trace_1}_${relay2Name};databus.relay.eventBuffer.trace.appendOnly=false;databus.relay.container.httpPort=${relay2Port};databus.relay.clusterManager.instanceName=${relay2Name};databus.relay.container.tcp.port=${relay2TCPPort};databus.relay.clusterManager.relayClusterName=${clusterName};databus.relay.clusterManager.relayZkConnectString=${RELAY_ZK_HOST}:${RELAY_ZK_PORT};databus.relay.dataSources.sequenceNumbersHandler.file.scnDir=${relay2_maxscn_dir}" -p ${espresso_conf_dir}/espresso_relay_cluster_manager_local.properties -l ${espresso_conf_dir}/espresso_relay_log4j2.properties --jvm_args="${jvm_args}"
echo '***** done creating second relay ***** '

## Wait for 60 sec (need more than 30 sec here)
sleep 120

## Sleep for 30 seconds to wait ZK notification to happen at the client
echo ==Check If Relay Puller is active at client 1
stat_txt="Test if Relay Puller Active at client 1"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o isRelayPullerActive --client_base_port_list=${client_port_1} --reg_id=${regId1}
source report_pass_fail.inc

## Sleep for 30 seconds to wait ZK notification to happen at the client
echo ==Check If Relay Puller is active at client 2
stat_txt="Test if Relay Puller Active at client 2"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o isRelayPullerActive --client_base_port_list=${client_port_2} --reg_id=${regId2}
source report_pass_fail.inc


curr_relay1=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getCurrentRelay --client_base_port_list=${client_port_1} --reg_id=${regId1}`
echo "Current Relay at client 1 is : ${curr_relay1}"

curr_relay2=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getCurrentRelay --client_base_port_list=${client_port_2} --reg_id=${regId2}`
echo "Current Relay at client 2 is : ${curr_relay2}"

# Write events
$SCRIPT_DIR/dbus2_gen_event.py --espresso_gen --espresso_data_file=${data_file} --espresso_db_name=${espressoDB_list} --espresso_table_name=IdNamePair --event_offset=100 --num_events=100 --event_per_sec=100 --espresso_host=${EspressoRouter} --espresso_port=${EspressoRouterPort}  

echo ==Sleeping for 30 secs to let relay pickup the events
sleep 30

## Wait for client catch up
echo ==Waiting for client to catchup at client 1
stat_txt="Waiting for client to catchup at client 1"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o wait_event_53  --expected_num_registrations=1  --db_list=${espressoDB_list} --db_range=1 --client_base_port_list=${client_port_1} --reg_id=${regId1} --debug
source report_pass_fail.inc

## Wait for client catch up
echo ==Waiting for client to catchup at client 2
stat_txt="Waiting for client to catchup at client 2"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o wait_event_53  --expected_num_registrations=1  --db_list=${espressoDB_list} --db_range=1 --client_base_port_list=${client_port_2} --reg_id=${regId2} --debug
source report_pass_fail.inc

######### TEARDOWN #########
sleep 60

#compare relay and client event logs
stat_txt="Test $0: Compare relay log and client1 log (after relay restart)"
echo "Test : Compare relay log and client1 log (after relay restart)"
$SCRIPT_DIR/dbus2_json_compare.py --espresso_cluster_compare --file1=${relay_event_trace_1} --file2=${consumer_1_log_1} --db_list=${espressoDB_list} --db_partitions=0 --client_host=${CLIENT_HOST} --client_port=${client_port_1} --master=True --slave=False --debug
source report_pass_fail.inc

echo "Test : Check that merged relay log"
$SCRIPT_DIR/dbus2_json_compare.py --cluster_merge --in=${relay_event_trace_1} --out=${relay_event_trace_merged_2} --db_list=${espressoDB_list} --db_range=1  --client_host=${CLIENT_HOST} --client_port=${client_port_1} --expected_event_count=200 --debug
source report_pass_fail.inc

stat_txt="Test $0: Compare relay log and client2 log (after relay restart)"
echo "Test : Compare relay log and client2 log (after relay restart)"
$SCRIPT_DIR/dbus2_json_compare.py --espresso_cluster_compare --file1=${relay_event_trace_1} --file2=${consumer_2_log_1} --db_list=${espressoDB_list} --db_partitions=0 --client_host=${CLIENT_HOST} --client_port=${client_port_2} --master=True --slave=False --compare_without_merge=True --debug
source report_pass_fail.inc

echo Stopping Clients...
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o stop

# Sleep for 10 sec
sleep 10

cp1_reg_dir="${consumer_1_cp_dir_from_root}/${regId1}"
cp2_reg_dir="${consumer_2_cp_dir_from_root}/${regId2}"

echo ==Check If checkpoint exit in ${cp1_reg_dir}
stat_txt="Test if checkpoint exit in ${cp1_reg_dir}"
if [ -e ${cp1_reg_dir}/cp_*current ]; then
  echo "exit 0" | bash  # success
else
  echo "exit -1" | bash  # fail
fi 
source report_pass_fail.inc

echo ==Check If checkpoint exit in ${cp2_reg_dir}
stat_txt="Test if checkpoint exit in ${cp2_reg_dir}"
if [ -e ${cp2_reg_dir}/cp_*current ]; then
  echo "exit 0" | bash  # success
else
  echo "exit -1" | bash  # fail
fi 
source report_pass_fail.inc


# Write events
$SCRIPT_DIR/dbus2_gen_event.py --espresso_gen --espresso_data_file=${data_file} --espresso_db_name=${espressoDB_list} --espresso_table_name=IdNamePair --event_offset=200 --num_events=100 --event_per_sec=100 --espresso_host=${EspressoRouter} --espresso_port=${EspressoRouterPort}  

# Sleep for the relay to catchup
sleep 10

#start the client and register it to receive events for EspressoDB all partitions
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o start --value_file=${consumer_1_value_log} --cmdline_props="databus.espresso.client.connectionDefaults.eventBuffer.trace.appendOnly=true;databus.espresso.testconsumer.appendOnly=true;databus.espresso.client.checkpointPersistence.fileSystem.rootDirectory=${consumer_1_cp_dir_from_root};databus.espresso.client.checkpointPersistence.clearBeforeUse=false;databus.espresso.client.connectionDefaults.eventBuffer.trace.filename=${consumer_1_log_1};databus.espresso.client.container.httpPort=${client_port_1};databus.espresso.client.clusterManager.relayClusterName=${clusterName};databus.espresso.client.clusterManager.enableDynamic=true;databus.espresso.client.subscriptions=${client_subs};databus.espresso.client.clusterManager.relayZkConnectString=${RELAY_ZK_HOST}:${RELAY_ZK_PORT};databus.espresso.client.connectionDefaults.dispatcherRetries.maxSleep=6000" -p ${espresso_conf_dir}/espresso_client_53.properties -l ${espresso_conf_dir}/espresso_client_log4j.properties  --jvm_args="${jvm_args}" 
echo '***** done creating first client ***** '
#start the client and register it to receive events for EspressoDB all partitions
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o start --value_file=${consumer_2_value_log} --cmdline_props="databus.espresso.client.connectionDefaults.eventBuffer.trace.appendOnly=true;databus.espresso.testconsumer.appendOnly=true;databus.espresso.client.checkpointPersistence.fileSystem.rootDirectory=${consumer_2_cp_dir_from_root};databus.espresso.client.checkpointPersistence.clearBeforeUse=false;databus.espresso.client.connectionDefaults.eventBuffer.trace.filename=${consumer_2_log_1};databus.espresso.client.container.httpPort=${client_port_2};databus.espresso.client.clusterManager.relayClusterName=${clusterName};databus.espresso.client.clusterManager.enableDynamic=true;databus.espresso.client.subscriptions=${client_subs};databus.espresso.client.clusterManager.relayZkConnectString=${RELAY_ZK_HOST}:${RELAY_ZK_PORT};databus.espresso.client.connectionDefaults.dispatcherRetries.maxSleep=6000" -p ${espresso_conf_dir}/espresso_client_53.properties -l ${espresso_conf_dir}/espresso_client_log4j.properties  --jvm_args="${jvm_args}" 
echo '***** done creating second client ***** '

# sleep for the client to start the puller threads
sleep 15

#Verify number of Registrations in client == number of partitions we specified
echo ==Verify Registrations for client 1
stat_txt="Verify Registrations for client 1"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o verifyRegistrations --expected_num_registrations=1 --client_base_port_list=${client_port_1}
source report_pass_fail.inc
echo ==Verify Registrations for client 2
stat_txt="Verify Registrations for client 2"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o verifyRegistrations --expected_num_registrations=1 --client_base_port_list=${client_port_2}
source report_pass_fail.inc

# Get RegId
regId1=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getRegistrationsByPhysicalPartition --expected_num_registrations=1 --client_base_port_list=${client_port_1} --db_name=${espressoDB_list} --partition_num=0`
echo "RegId1 is : ${regId1}"
regId2=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getRegistrationsByPhysicalPartition --expected_num_registrations=1 --client_base_port_list=${client_port_2} --db_name=${espressoDB_list} --partition_num=0`
echo "RegId2 is : ${regId2}"

# check if Relay Puller is active
echo ==Check If Relay Puller is active for client 1
stat_txt="Test if Relay Puller Active for client 1"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o isRelayPullerActive --client_base_port_list=${client_port_1} --reg_id=${regId1}
source report_pass_fail.inc
echo ==Check If Relay Puller is active for client 2
stat_txt="Test if Relay Puller Active for client 2"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o isRelayPullerActive --client_base_port_list=${client_port_2} --reg_id=${regId2}
source report_pass_fail.inc

curr_relay1=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getCurrentRelay  --client_base_port_list=${client_port_1} --reg_id=${regId1}`
echo "Current Relay for client 1 is : ${curr_relay1}"
curr_relay1=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getCurrentRelay  --client_base_port_list=${client_port_2} --reg_id=${regId2}`
echo "Current Relay for client 1 is : ${curr_relay2}"

## Wait for client catch up
echo ==Waiting for client1 to catchup
stat_txt="Waiting for client2 to catchup "
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o wait_event_53  --expected_num_registrations=1 --db_list=${espressoDB_list} --db_range=1 --client_base_port_list=${client_port_1} --reg_id=${regId1} --debug
source report_pass_fail.inc
echo ==Waiting for client2 to catchup
stat_txt="Waiting for client2 to catchup "
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o wait_event_53 --expected_num_registrations=1 --db_list=${espressoDB_list} --db_range=1 --client_base_port_list=${client_port_2} --reg_id=${regId2} --debug
source report_pass_fail.inc

## Wait for 30 sec
sleep 30

#compare relay and client event logs
stat_txt="Test $0: Compare relay log and client1 log (after client restart)"
echo "Test : Compare relay log and client1 log (after client restart)"
#$SCRIPT_DIR/dbus2_json_compare.py --espresso_compare --file1=${file1} --file2=${consumer_1_log_1} --db_list=${espressoDB_list} --db_range=1
# Now Client logs will have 300 events whereas relay will have only 200 events(since they have been restarted with no append). We need to compare last 200 elements of client logs with relay log
$SCRIPT_DIR/dbus2_json_compare.py --espresso_cluster_compare --file1=${relay_event_trace_1} --file2=${consumer_1_log_1} --db_list=${espressoDB_list} --db_partitions=0 --client_host=${CLIENT_HOST} --client_port=${client_port_1} --master=True --slave=False --append=False --file2_offset=100 --debug
source report_pass_fail.inc

echo "Test : Check the merged relay log (after client restart)"
echo "Test : Check the merged relay log (after client restart)"
#$SCRIPT_DIR/dbus2_json_compare.py --merge --in=${file1} --out=${relay_event_trace_merged} --db_list=${espressoDB_list} --db_range=1
# Note the expected_event is set to 200 (and not 300) only as the relay is not restarted ( so the relay event trace would have appended new events). After last restart of the relay, 200 events have been produced. The append=False will ensure that the check is for the current run of the relay <== All this because of the way comparison is done using a statelss driver script.
$SCRIPT_DIR/dbus2_json_compare.py --cluster_merge --in=${relay_event_trace_1} --out=${relay_event_trace_merged_4} --db_list=${espressoDB_list} --db_range=1  --client_host=${CLIENT_HOST} --client_port=${client_port_1} --append=False --expected_event_count=200 --debug
source report_pass_fail.inc

#compare relay and client event logs
stat_txt="Test $0: Compare relay log and client2 log (after client restart)"
echo "Test : Compare relay log and client2 log (after client restart)"
# Now Client logs will have 300 events whereas relay will have only 200 events(since they have been restarted with no append). We need to compare last 200 elements of client logs with relay log
$SCRIPT_DIR/dbus2_json_compare.py --espresso_cluster_compare --file1=${relay_event_trace_1} --file2=${consumer_2_log_1} --db_list=${espressoDB_list} --db_partitions=0 --client_host=${CLIENT_HOST} --client_port=${client_port_2} --master=True --slave=False --compare_without_merge=True  --file2_offset=100 --debug 
source report_pass_fail.inc


$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o stop
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_relay -o stop

stat_txt="Test $0: Compare Value dumps for client 1"
echo "Test : Compare value dumps for client 1"
$SCRIPT_DIR/dbus2_json_compare.py --espresso_value_compare --file1=${data_file} --file2=${consumer_1_value_log} --db_list=${espressoDB_list} --db_range=1 --espresso_host=${EspressoRouter} --espresso_port=${EspressoRouterPort} --espresso_key_range=300
source report_pass_fail.inc

stat_txt="Test $0: Compare Value dumps for client 2"
echo "Test : Compare value dumps for client 2"
final_report=1
$SCRIPT_DIR/dbus2_json_compare.py --espresso_value_compare --file1=${data_file} --file2=${consumer_2_value_log} --db_list=${espressoDB_list} --db_range=1 --espresso_host=${EspressoRouter} --espresso_port=${EspressoRouterPort} --espresso_key_range=300
source report_pass_fail.inc

#$SCRIPT_DIR/dbus2_json_compare.py --merge --in=${file1} --out=${relay_event_trace_merged_2} --db_list=${espressoDB_list} --db_range=1
######### TEARDOWN COMPLETE #########
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o invoke --cluster_manager_props="--dropCluster ${clusterName} --zkSvr ${RELAY_ZK_HOST}:${RELAY_ZK_PORT}"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_admin_client -o stop
# Teardown espresso setup
$SCRIPT_DIR/dbus2_driver.py -n ${TEST_NAME} -c cluster_manager -o teardown


#echo == Looking for Error,ERROR or Exception in log files
#ls -1tr $LOG_DIR/esp* | xargs grep -i Exception
exit $all_stat
