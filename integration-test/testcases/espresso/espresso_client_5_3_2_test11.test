#!/bin/bash
#******************************************************
# Test Case to test the checkpointing progress of clients when clients get restarted
# In this case, the storage nodes maintain the same role (Master/slave) and no rebalancing occurs
# StorageNode Setup with 1 Partition EspressoDB with 1 Master and 1 Slave
# Master - Node 1
# Slave - Node 2
#  1. Setup Storage Nodes and cluster
#  2. Start Relay Cluster and Start Relays
#  3. Start 2 clients client1 and client2 with a subscription to EspressoDB partition
#  4. Generate Data to the Storage Node
#  5. Verify Data at the Clients and their states
#  6. Stop Relays
#  7. Verify Client is suspended as there are no more relays ( this ensures dynamic notification happens as without this 
#                                          client will not go to Suspended state (since retry is set to -1 (never stop)))
#  8. Start Relays 
#  9. Verify clients are back to active.
# 10. Generate Data to the Storage Node
# 11. Verify all the data is present at the clients ( compare with both relay and data file )
# 12. Now restart the clients (with checkpoint.clearBrforeUse=false 
# 13. Genereate more data to the storage node and verify all the data is present at the clients
# 
# set TEST_NAME before calling setup_env.inc
#******************************************************
export TEST_NAME=espresso_client_5_3_2_test11

# sets up common environmnet variables and 
source setup_env2.inc
source ${SCRIPT_DIR}/test_common.inc
#***************************************************************************************************************************************
#all ${ALL_CAPS} type vars come from setup_env.inc(except TEST_NAME)...check that file first before introducing any new variables here
#***************************************************************************************************************************************
espressoDB_list=EspressoDB  	#Can be a comma-separated list
espressoDB_range=1     	#Can be a comma-separated list
espressoDB_replicas=2 	#Can be a comma-separated list
clusterName=relayIntTemp
jvm_direct_memory="-XX:MaxDirectMemorySize=200m"
jvm_min_heap="-Xms100m"
jvm_max_heap="-Xmx100m"
jvm_gc_args="-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:-CMSParallelRemarkEnabled -XX:MaxTenuringThreshold=1 -XX:SurvivorRatio=3 -XX:CMSInitiatingOccupancyFraction=85 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintTenuringDistribution"
jvm_args="${jvm_direct_memory} ${jvm_min_heap} ${jvm_max_heap} ${jvm_new_size} ${gvm_gc_args}"
let client_port_1="${CLIENT_PORT_BASE}+2"
let client_port_2="${CLIENT_PORT_BASE}+3"
relay_event_trace_1=${WORK_DIR_FROM_ROOT}/espresso_relay_event_trace
relay_event_trace_merged=${WORK_DIR_FROM_ROOT}/espresso_relay_event_trace_merged
relay_event_trace_merged_2=${WORK_DIR_FROM_ROOT}/espresso_relay_event_trace_merged_2
relay_event_trace_merged_3=${WORK_DIR_FROM_ROOT}/espresso_relay_event_trace_merged_3
relay_event_trace_merged_4=${WORK_DIR_FROM_ROOT}/espresso_relay_event_trace_merged_4
relay_event_trace_merged_5=${WORK_DIR_FROM_ROOT}/espresso_relay_event_trace_merged_5
consumer_1_log_1=${WORK_DIR_FROM_ROOT}/espresso_consumer_1.events.1
consumer_1_value_log=${WORK_DIR_FROM_ROOT}/espresso_consumer_1.values
consumer_2_log_1=${WORK_DIR_FROM_ROOT}/espresso_consumer_2.events.1
consumer_2_value_log=${WORK_DIR_FROM_ROOT}/espresso_consumer_2.values
consumer_1_cp_dir=${WORK_DIR_FROM_ROOT}/ckpt/1
consumer_2_cp_dir=${WORK_DIR_FROM_ROOT}/ckpt/2
consumer_1_cp_dir_from_root=${VIEW_ROOT}/${WORK_DIR_FROM_ROOT}/ckpt/1
consumer_2_cp_dir_from_root=${VIEW_ROOT}/${WORK_DIR_FROM_ROOT}/ckpt/2
espresso_conf_dir=${CONFIG_DIR}/espresso
espressoDB_config_file_base=${espresso_conf_dir}/json/ppart__es_EspressoDB
client_subs=espresso://MASTER/EspressoDB/0/*
data_root=${VIEW_ROOT}/integration-test/testcases/espresso/data
data_file=${data_root}/EspressoDB_Random1.dat

relay1_maxscn_dir=${VIEW_ROOT}/${WORK_DIR_FROM_ROOT}/maxScn1
relay1_mmap_dir=${VIEW_ROOT}/${WORK_DIR_FROM_ROOT}/mmap1
relay2_maxscn_dir=${VIEW_ROOT}/${WORK_DIR_FROM_ROOT}/maxScn2
relay2_mmap_dir=${VIEW_ROOT}/${WORK_DIR_FROM_ROOT}/mmap2


######### SETUP #########
# Uncomment this if you want the relay cluster and storage node clusters to be hosted on separate zookeeper instances
# export RELAY_ZK_PORT=2183

LOG_INFO Setup espresso components
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o stop
$SCRIPT_DIR/dbus2_driver.py -n ${TEST_NAME} -c cluster_manager -o setup --db_list "${espressoDB_list}" --db_range "${espressoDB_range}" --db_replicas "${espressoDB_replicas}" --zookeeper_server_hosts=${ZK_HOST} --zookeeper_server_ports=${ZK_PORT} --helix_clustername=${STORAGE_NODE_CLUSTER} --relay_zookeeper_server_hosts=${RELAY_ZK_HOST} --relay_zookeeper_server_ports=${RELAY_ZK_PORT}
LOG_INFO '***** done setting up espresso components *****'

LOG_INFO Create the directories
CREATE_CLEAN_DIR ${consumer_1_cp_dir_from_root}
CREATE_CLEAN_DIR ${consumer_2_cp_dir_from_root}
CREATE_CLEAN_DIR ${relay1_maxscn_dir}
CREATE_CLEAN_DIR ${relay1_mmap_dir}
CREATE_CLEAN_DIR ${relay2_maxscn_dir}
CREATE_CLEAN_DIR ${relay2_mmap_dir}

TEST_STEP Setup Relay Cluster
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o invoke --cluster_manager_props="--addCluster ${clusterName} --zkSvr ${RELAY_ZK_HOST}:${RELAY_ZK_PORT}"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o invoke --cluster_manager_props="--listClusters --zkSvr ${RELAY_ZK_HOST}:${RELAY_ZK_PORT}" 
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o invoke --cluster_manager_props="--addNode ${clusterName} localhost:${relay1Port} --zkSvr ${RELAY_ZK_HOST}:${RELAY_ZK_PORT}"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o invoke --cluster_manager_props="--addNode ${clusterName} localhost:${relay2Port} --zkSvr ${RELAY_ZK_HOST}:${RELAY_ZK_PORT}"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o invoke --cluster_manager_props="--addResource ${clusterName} relayLeaderStandby 1 LeaderStandby --zkSvr ${RELAY_ZK_HOST}:${RELAY_ZK_PORT}"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o invoke --cluster_manager_props="--rebalance ${clusterName} relayLeaderStandby 2 --zkSvr ${RELAY_ZK_HOST}:${RELAY_ZK_PORT}" 
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o invoke --cluster_manager_props="--addResource ${clusterName} ${espressoDB_list} ${espressoDB_range} OnlineOffline --zkSvr ${RELAY_ZK_HOST}:${RELAY_ZK_PORT}" 
REPORT_TEST_STEP report_pass_fail.inc

LOG_INFO '***** done creating relay cluster ***** '

LOG_INFO Start helix controller for relay cluster
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o start --helix_clustername=${clusterName} --zookeeper_server_hosts=${RELAY_ZK_HOST} --zookeeper_server_ports=${RELAY_ZK_PORT}
LOG_INFO '***** done starting helix controller for relay cluster ***** '

LOG_INFO start the 2 relays. Configure them to recieve events from the EspressoDB schema. Register it to listen to the EV of the DevCluster_Dbus Espresso Storage Node Cluster 
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_relay -o start --db_config_file=${espressoDB_config_file_base} --db_config_file_range=${espressoDB_range} --cmdline_props="databus.relay.eventBuffer.trace.filename=${relay_event_trace_1}_${relay1Name};databus.relay.eventBuffer.trace.appendOnly=false;databus.relay.container.httpPort=${relay1Port};databus.relay.clusterManager.instanceName=${relay1Name};databus.relay.container.tcp.port=${relay1TCPPort};databus.relay.clusterManager.relayClusterName=${clusterName};databus.relay.clusterManager.relayZkConnectString=${RELAY_ZK_HOST}:${RELAY_ZK_PORT};databus.relay.dataSources.sequenceNumbersHandler.file.scnDir=${relay1_maxscn_dir};databus.relay.eventBuffer.mmapDirectory=${relay1_mmap_dir}" -p ${espresso_conf_dir}/espresso_relay_5_3_2.properties -l ${espresso_conf_dir}/espresso_relay_log4j.properties --jvm_args="${jvm_args}" 
LOG_INFO '***** done creating first relay ***** '

$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_relay -o start --db_config_file=${espressoDB_config_file_base} --db_config_file_range=${espressoDB_range} --cmdline_props="databus.relay.eventBuffer.trace.filename=${relay_event_trace_1}_${relay2Name};databus.relay.eventBuffer.trace.appendOnly=false;databus.relay.container.httpPort=${relay2Port};databus.relay.clusterManager.instanceName=${relay2Name};databus.relay.container.tcp.port=${relay2TCPPort};databus.relay.clusterManager.relayClusterName=${clusterName};databus.relay.clusterManager.relayZkConnectString=${RELAY_ZK_HOST}:${RELAY_ZK_PORT};databus.relay.dataSources.sequenceNumbersHandler.file.scnDir=${relay2_maxscn_dir};databus.relay.eventBuffer.mmapDirectory=${relay2_mmap_dir}" -p ${espresso_conf_dir}/espresso_relay_5_3_2.properties -l ${espresso_conf_dir}/espresso_relay_log4j2.properties --jvm_args="${jvm_args}"
LOG_INFO '***** done creating second relay ***** '

sleep 20

LOG_INFO Write events
$SCRIPT_DIR/dbus2_gen_event.py --espresso_gen --espresso_data_file=${data_file} --espresso_db_name=${espressoDB_list} --espresso_table_name=IdNamePair --num_events=100 --event_per_sec=100 --espresso_host=${EspressoRouter} --espresso_port=${EspressoRouterPort} 

LOG_INFO Sleep for the relay to catchup
sleep 10

LOG_INFO start the client and register it to receive events for EspressoDB all partitions
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o start --value_file=${consumer_1_value_log} --cmdline_props="databus.espresso.client.checkpointPersistence.fileSystem.rootDirectory=${consumer_1_cp_dir_from_root};databus.espresso.client.checkpointPersistence.clearBeforeUse=true;databus.espresso.client.connectionDefaults.eventBuffer.trace.filename=${consumer_1_log_1};databus.espresso.client.container.httpPort=${client_port_1};databus.espresso.client.clusterManager.relayClusterName=${clusterName};databus.espresso.client.clusterManager.enableDynamic=true;databus.espresso.client.subscriptions=${client_subs};databus.espresso.client.clusterManager.relayZkConnectString=${RELAY_ZK_HOST}:${RELAY_ZK_PORT};databus.espresso.client.connectionDefaults.dispatcherRetries.maxSleep=6000" -p ${espresso_conf_dir}/espresso_client_53.properties -l ${espresso_conf_dir}/espresso_client_log4j.properties  --jvm_args="${jvm_args}" 
LOG_INFO '***** done creating first client ***** '
#start the client and register it to receive events for EspressoDB all partitions
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o start --value_file=${consumer_2_value_log} --cmdline_props="databus.espresso.client.checkpointPersistence.fileSystem.rootDirectory=${consumer_2_cp_dir_from_root};databus.espresso.client.checkpointPersistence.clearBeforeUse=true;databus.espresso.client.connectionDefaults.eventBuffer.trace.filename=${consumer_2_log_1};databus.espresso.client.container.httpPort=${client_port_2};databus.espresso.client.clusterManager.relayClusterName=${clusterName};databus.espresso.client.clusterManager.enableDynamic=true;databus.espresso.client.subscriptions=${client_subs};databus.espresso.client.clusterManager.relayZkConnectString=${RELAY_ZK_HOST}:${RELAY_ZK_PORT};databus.espresso.client.connectionDefaults.dispatcherRetries.maxSleep=6000" -p ${espresso_conf_dir}/espresso_client_53.properties -l ${espresso_conf_dir}/espresso_client_log4j.properties  --jvm_args="${jvm_args}" 
LOG_INFO '***** done creating second client ***** '

LOG_INFO sleep for the client to start the puller threads
sleep 15

TEST_STEP Verify Registrations for client 1
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o verifyRegistrations --expected_num_registrations=1 --client_base_port_list=${client_port_1}
REPORT_TEST_STEP
TEST_STEP Verify Registrations for client 2
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o verifyRegistrations --expected_num_registrations=1 --client_base_port_list=${client_port_2}
REPORT_TEST_STEP

LOG_INFO Get RegId
regId1=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getRegistrationsByPhysicalPartition --expected_num_registrations=1 --client_base_port_list=${client_port_1} --db_name=${espressoDB_list} --partition_num=0`
LOG_INFO "RegId1 is : ${regId1}"
regId2=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getRegistrationsByPhysicalPartition --expected_num_registrations=1 --client_base_port_list=${client_port_2} --db_name=${espressoDB_list} --partition_num=0`
LOG_INFO "RegId2 is : ${regId2}"

TEST_STEP check if Relay Puller is active
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o isRelayPullerActive --client_base_port_list=${client_port_1} --reg_id=${regId1} --debug
REPORT_TEST_STEP
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o isRelayPullerActive --client_base_port_list=${client_port_2} --reg_id=${regId2} --debug
REPORT_TEST_STEP


curr_relay1=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getCurrentRelay  --client_base_port_list=${client_port_1} --reg_id=${regId1}`
LOG_INFO "Current Relay for client 1 is : ${curr_relay1}"
curr_relay1=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getCurrentRelay  --client_base_port_list=${client_port_2} --reg_id=${regId2}`
LOG_INFO "Current Relay for client 1 is : ${curr_relay2}"

TEST_STEP Wait for client catch up
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o wait_event_53  --expected_num_registrations=1 --db_list=${espressoDB_list} --db_range=1 --client_base_port_list=${client_port_1} --reg_id=${regId1} --debug
REPORT_TEST_STEP
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o wait_event_53 --expected_num_registrations=1 --db_list=${espressoDB_list} --db_range=1 --client_base_port_list=${client_port_2} --reg_id=${regId2} --debug
REPORT_TEST_STEP

LOG_INFO Wait for 30 sec
sleep 30

TEST_STEP compare relay and client event logs
#$SCRIPT_DIR/dbus2_json_compare.py --espresso_compare --file1=${file1} --file2=${consumer_1_log_1} --db_list=${espressoDB_list} --db_range=1
$SCRIPT_DIR/dbus2_json_compare.py --espresso_cluster_compare --file1=${relay_event_trace_1} --file2=${consumer_1_log_1} --db_list=${espressoDB_list} --db_partitions=0 --client_host=${CLIENT_HOST} --client_port=${client_port_1} --master=True --slave=False --append=False --debug
REPORT_TEST_STEP

TEST_STEP Check the merged relay log
$SCRIPT_DIR/dbus2_json_compare.py --cluster_merge --in=${relay_event_trace_1} --out=${relay_event_trace_merged_2} --db_list=${espressoDB_list} --db_range=1  --client_host=${CLIENT_HOST} --client_port=${client_port_1} --append=False --expected_event_count=100 --debug
REPORT_TEST_STEP

TEST_STEP compare relay and client event logs
$SCRIPT_DIR/dbus2_json_compare.py --espresso_cluster_compare --file1=${relay_event_trace_1} --file2=${consumer_2_log_1} --db_list=${espressoDB_list} --db_partitions=0 --client_host=${CLIENT_HOST} --client_port=${client_port_2} --master=True --slave=False --compare_without_merge=True --debug
REPORT_TEST_STEP

LOG_INFO Now stop the relay  
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_relay -o stop

LOG_INFO Wait for 60 sec 
sleep 60

curr_time=`date`
LOG_INFO "Current Time :${curr_time}"

TEST_STEP Test if Relay Puller is Inactive at client 1
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o isRelayPullerInactive --client_base_port_list=${client_port_1} --reg_id=${regId1} --debug
REPORT_TEST_STEP
TEST_STEP Test if Relay Puller is Inactive at client 2
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o isRelayPullerInactive --client_base_port_list=${client_port_2} --reg_id=${regId2} --debug
REPORT_TEST_STEP


LOG_INFO start the 2 relays. Configure them to recieve events from the EspressoDB schema. Register it to listen to the EV of the DevCluster_Dbus Espresso Storage Node Cluster 
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_relay -o start --db_config_file=${espressoDB_config_file_base} --db_config_file_range=${espressoDB_range} --cmdline_props="databus.relay.eventBuffer.trace.filename=${relay_event_trace_1}_${relay1Name};databus.relay.eventBuffer.trace.appendOnly=false;databus.relay.container.httpPort=${relay1Port};databus.relay.clusterManager.instanceName=${relay1Name};databus.relay.container.tcp.port=${relay1TCPPort};databus.relay.clusterManager.relayClusterName=${clusterName};databus.relay.clusterManager.relayZkConnectString=${RELAY_ZK_HOST}:${RELAY_ZK_PORT};databus.relay.dataSources.sequenceNumbersHandler.file.scnDir=${relay1_maxscn_dir};databus.relay.eventBuffer.mmapDirectory=${relay1_mmap_dir}" -p ${espresso_conf_dir}/espresso_relay_5_3_2.properties -l ${espresso_conf_dir}/espresso_relay_log4j.properties --jvm_args="${jvm_args}" 
LOG_INFO '***** done creating first relay ***** '

$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_relay -o start --db_config_file=${espressoDB_config_file_base} --db_config_file_range=${espressoDB_range} --cmdline_props="databus.relay.eventBuffer.trace.filename=${relay_event_trace_1}_${relay2Name};databus.relay.eventBuffer.trace.appendOnly=false;databus.relay.container.httpPort=${relay2Port};databus.relay.clusterManager.instanceName=${relay2Name};databus.relay.container.tcp.port=${relay2TCPPort};databus.relay.clusterManager.relayClusterName=${clusterName};databus.relay.clusterManager.relayZkConnectString=${RELAY_ZK_HOST}:${RELAY_ZK_PORT};databus.relay.dataSources.sequenceNumbersHandler.file.scnDir=${relay2_maxscn_dir};databus.relay.eventBuffer.mmapDirectory=${relay2_mmap_dir}" -p ${espresso_conf_dir}/espresso_relay_5_3_2.properties -l ${espresso_conf_dir}/espresso_relay_log4j2.properties --jvm_args="${jvm_args}"
LOG_INFO '***** done creating second relay ***** '

LOG_INFO Wait for 60 sec 
sleep 60 


TEST_STEP Test if Relay Puller Active at client 1
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o isRelayPullerActive --client_base_port_list=${client_port_1} --reg_id=${regId1}
REPORT_TEST_STEP

TEST_STEP Test if Relay Puller Active at client 2
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o isRelayPullerActive --client_base_port_list=${client_port_2} --reg_id=${regId2}
REPORT_TEST_STEP

curr_relay1=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getCurrentRelay --client_base_port_list=${client_port_1} --reg_id=${regId1}`
LOG_INFO "Current Relay at client 1 is : ${curr_relay1}"

curr_relay2=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getCurrentRelay --client_base_port_list=${client_port_2} --reg_id=${regId2}`
LOG_INFO "Current Relay at client 2 is : ${curr_relay2}"

LOG_INFO Write events
$SCRIPT_DIR/dbus2_gen_event.py --espresso_gen --espresso_data_file=${data_file} --espresso_db_name=${espressoDB_list} --espresso_table_name=IdNamePair --event_offset=100 --num_events=100 --event_per_sec=100 --espresso_host=${EspressoRouter} --espresso_port=${EspressoRouterPort}  

LOG_INFO Sleeping for 30 secs to let relay pickup the events
sleep 30

TEST_STEP Waiting for client to catchup at client 1
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o wait_event_53  --expected_num_registrations=1  --db_list=${espressoDB_list} --db_range=1 --client_base_port_list=${client_port_1} --reg_id=${regId1} --debug
REPORT_TEST_STEP

TEST_STEP Waiting for client to catchup at client 2
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o wait_event_53  --expected_num_registrations=1  --db_list=${espressoDB_list} --db_range=1 --client_base_port_list=${client_port_2} --reg_id=${regId2} --debug
REPORT_TEST_STEP

LOG_INFO Sleep 20
sleep 20

TEST_STEP compare relay and client event logs
$SCRIPT_DIR/dbus2_json_compare.py --espresso_cluster_compare --file1=${relay_event_trace_1} --file2=${consumer_1_log_1} --db_list=${espressoDB_list} --db_partitions=0 --client_host=${CLIENT_HOST} --client_port=${client_port_1} --master=True --slave=False --debug
REPORT_TEST_STEP

TEST_STEP Test : Check that merged relay log
$SCRIPT_DIR/dbus2_json_compare.py --cluster_merge --in=${relay_event_trace_1} --out=${relay_event_trace_merged_2} --db_list=${espressoDB_list} --db_range=1  --client_host=${CLIENT_HOST} --client_port=${client_port_1} --expected_event_count=200 --debug
REPORT_TEST_STEP

TEST_STEP Compare relay log and client2 log 
$SCRIPT_DIR/dbus2_json_compare.py --espresso_cluster_compare --file1=${relay_event_trace_1} --file2=${consumer_2_log_1} --db_list=${espressoDB_list} --db_partitions=0 --client_host=${CLIENT_HOST} --client_port=${client_port_2} --master=True --slave=False --compare_without_merge=True --debug
REPORT_TEST_STEP

LOG_INFO Stopping Clients...
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o stop

# Sleep for 10 sec
sleep 10

cp1_reg_dir="${consumer_1_cp_dir_from_root}/${regId1}"
cp2_reg_dir="${consumer_2_cp_dir_from_root}/${regId2}"

files1=$(ls ${cp1_reg_dir}/cp_*current 2>/dev/null | wc -l)
files2=$(ls ${cp2_reg_dir}/cp_*current 2>/dev/null | wc -l)
TEST_STEP "Test if checkpoint exist in ${cp1_reg_dir}"
if [ **"$files1" != "0"** ]; then
  echo "exit 0" | bash  # success
else
  echo "exit -1" | bash  # fail
fi 
REPORT_TEST_STEP


TEST_STEP "Test if checkpoint exist in ${cp2_reg_dir}"
if [ **"$files2" != "0"** ]; then
  echo "exit 0" | bash  # success
else
  echo "exit -1" | bash  # fail
fi 
REPORT_TEST_STEP

LOG_INFO Write events
$SCRIPT_DIR/dbus2_gen_event.py --espresso_gen --espresso_data_file=${data_file} --espresso_db_name=${espressoDB_list} --espresso_table_name=IdNamePair --event_offset=200 --num_events=100 --event_per_sec=100 --espresso_host=${EspressoRouter} --espresso_port=${EspressoRouterPort}  

LOG_INFO Sleep for the relay to catchup
sleep 10

LOG_INFO start the client and register it to receive events for EspressoDB all partitions
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o start --value_file=${consumer_1_value_log} --cmdline_props="databus.espresso.client.connectionDefaults.eventBuffer.trace.appendOnly=true;databus.espresso.testconsumer.appendOnly=true;databus.espresso.client.checkpointPersistence.fileSystem.rootDirectory=${consumer_1_cp_dir_from_root};databus.espresso.client.checkpointPersistence.clearBeforeUse=false;databus.espresso.client.connectionDefaults.eventBuffer.trace.filename=${consumer_1_log_1};databus.espresso.client.container.httpPort=${client_port_1};databus.espresso.client.clusterManager.relayClusterName=${clusterName};databus.espresso.client.clusterManager.enableDynamic=true;databus.espresso.client.subscriptions=${client_subs};databus.espresso.client.clusterManager.relayZkConnectString=${RELAY_ZK_HOST}:${RELAY_ZK_PORT};databus.espresso.client.connectionDefaults.dispatcherRetries.maxSleep=6000" -p ${espresso_conf_dir}/espresso_client_53.properties -l ${espresso_conf_dir}/espresso_client_log4j.properties  --jvm_args="${jvm_args}" 
LOG_INFO '***** done creating first client ***** '

LOG_INFO start the client and register it to receive events for EspressoDB all partitions
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o start --value_file=${consumer_2_value_log} --cmdline_props="databus.espresso.client.connectionDefaults.eventBuffer.trace.appendOnly=true;databus.espresso.testconsumer.appendOnly=true;databus.espresso.client.checkpointPersistence.fileSystem.rootDirectory=${consumer_2_cp_dir_from_root};databus.espresso.client.checkpointPersistence.clearBeforeUse=false;databus.espresso.client.connectionDefaults.eventBuffer.trace.filename=${consumer_2_log_1};databus.espresso.client.container.httpPort=${client_port_2};databus.espresso.client.clusterManager.relayClusterName=${clusterName};databus.espresso.client.clusterManager.enableDynamic=true;databus.espresso.client.subscriptions=${client_subs};databus.espresso.client.clusterManager.relayZkConnectString=${RELAY_ZK_HOST}:${RELAY_ZK_PORT};databus.espresso.client.connectionDefaults.dispatcherRetries.maxSleep=6000" -p ${espresso_conf_dir}/espresso_client_53.properties -l ${espresso_conf_dir}/espresso_client_log4j.properties  --jvm_args="${jvm_args}" 
LOG_INFO '***** done creating second client ***** '

LOG_INFO sleep for the client to start the puller threads
sleep 15

TEST_STEP Verify number of Registrations in client == number of partitions we specified
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o verifyRegistrations --expected_num_registrations=1 --client_base_port_list=${client_port_1}
REPORT_TEST_STEP
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o verifyRegistrations --expected_num_registrations=1 --client_base_port_list=${client_port_2}
REPORT_TEST_STEP

LOG_INFO Get RegId
regId1=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getRegistrationsByPhysicalPartition --expected_num_registrations=1 --client_base_port_list=${client_port_1} --db_name=${espressoDB_list} --partition_num=0`
LOG_INFO "RegId1 is : ${regId1}"
regId2=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getRegistrationsByPhysicalPartition --expected_num_registrations=1 --client_base_port_list=${client_port_2} --db_name=${espressoDB_list} --partition_num=0`
LOG_INFO "RegId2 is : ${regId2}"

TEST_STEP check if Relay Puller is active
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o isRelayPullerActive --client_base_port_list=${client_port_1} --reg_id=${regId1}
REPORT_TEST_STEP
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o isRelayPullerActive --client_base_port_list=${client_port_2} --reg_id=${regId2}
REPORT_TEST_STEP

curr_relay1=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getCurrentRelay  --client_base_port_list=${client_port_1} --reg_id=${regId1}`
LOG_INFO "Current Relay for client 1 is : ${curr_relay1}"
curr_relay1=`$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o getCurrentRelay  --client_base_port_list=${client_port_2} --reg_id=${regId2}`
LOG_INFO "Current Relay for client 1 is : ${curr_relay2}"

TEST_STEP Wait for client catch up
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o wait_event_53  --expected_num_registrations=1 --db_list=${espressoDB_list} --db_range=1 --client_base_port_list=${client_port_1} --reg_id=${regId1} --debug
REPORT_TEST_STEP
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o wait_event_53 --expected_num_registrations=1 --db_list=${espressoDB_list} --db_range=1 --client_base_port_list=${client_port_2} --reg_id=${regId2} --debug
REPORT_TEST_STEP

LOG_INFO Wait for 30 sec
sleep 30

TEST_STEP compare relay and client event logs
# Now Client logs will have 300 events whereas relay will have only 200 events(since they have been restarted with no append). We need to compare last 200 elements of client logs with relay log
$SCRIPT_DIR/dbus2_json_compare.py --espresso_cluster_compare --file1=${relay_event_trace_1} --file2=${consumer_1_log_1} --db_list=${espressoDB_list} --db_partitions=0 --client_host=${CLIENT_HOST} --client_port=${client_port_1} --master=True --slave=False --append=False --file2_offset=100 --debug
REPORT_TEST_STEP

TEST_STEP Check the merged relay log 
# Note the expected_event is set to 200 (and not 300) only as the relay is not restarted ( so the relay event trace would have appended new events). After last restart of the relay, 200 events have been produced. The append=False will ensure that the check is for the current run of the relay <== All this because of the way comparison is done using a statelss driver script.
$SCRIPT_DIR/dbus2_json_compare.py --cluster_merge --in=${relay_event_trace_1} --out=${relay_event_trace_merged_4} --db_list=${espressoDB_list} --db_range=1  --client_host=${CLIENT_HOST} --client_port=${client_port_1} --append=False --expected_event_count=200 --debug
REPORT_TEST_STEP

TEST_STEP compare relay and client event logs
# Now Client logs will have 300 events whereas relay will have only 200 events(since they have been restarted with no append). We need to compare last 200 elements of client logs with relay log
$SCRIPT_DIR/dbus2_json_compare.py --espresso_cluster_compare --file1=${relay_event_trace_1} --file2=${consumer_2_log_1} --db_list=${espressoDB_list} --db_partitions=0 --client_host=${CLIENT_HOST} --client_port=${client_port_2} --master=True --slave=False --compare_without_merge=True  --file2_offset=100 --debug 
REPORT_TEST_STEP


$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_client -o stop
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c espresso_relay -o stop

TEST_STEP Compare value dumps for client 1
$SCRIPT_DIR/dbus2_json_compare.py --espresso_value_compare --file1=${data_file} --file2=${consumer_1_value_log} --db_list=${espressoDB_list} --db_range=1 --espresso_host=${EspressoRouter} --espresso_port=${EspressoRouterPort} --espresso_key_range=300
REPORT_TEST_STEP

LOG_INFO Compare value dumps for client 2
$SCRIPT_DIR/dbus2_json_compare.py --espresso_value_compare --file1=${data_file} --file2=${consumer_2_value_log} --db_list=${espressoDB_list} --db_range=1 --espresso_host=${EspressoRouter} --espresso_port=${EspressoRouterPort} --espresso_key_range=300
REPORT_TEST_STEP

$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_manager -o invoke --cluster_manager_props="--dropCluster ${clusterName} --zkSvr ${RELAY_ZK_HOST}:${RELAY_ZK_PORT}"
$SCRIPT_DIR/dbus2_driver.py -n $TEST_NAME -c cluster_admin_client -o stop
# Teardown espresso setup
$SCRIPT_DIR/dbus2_driver.py -n ${TEST_NAME} -c cluster_manager -o teardown

LOG_INFO ######### TEARDOWN COMPLETE #########

FINAL_TEST_REPORT

exit $all_stat
